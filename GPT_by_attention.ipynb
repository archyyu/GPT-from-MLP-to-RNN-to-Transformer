{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/archyyu/GPT-from-MLP-to-RNN-to-Transformer/blob/main/GPT_by_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ma80RtZJnLKK"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMBvKgnEpZmH",
        "outputId": "4d3a6c43-ea29-4c38-f75c-bc95e622a4e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data has 1115394 characters, 65 unique.\n"
          ]
        }
      ],
      "source": [
        "# Data I/O\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "response = requests.get(url)\n",
        "data = response.text\n",
        "\n",
        "chars = list(set(data))\n",
        "data_size, vocab_size = len(data), len(chars)\n",
        "print(f'data has {data_size} characters, {vocab_size} unique.')\n",
        "\n",
        "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
        "ix_to_char = {i: ch for i, ch in enumerate(chars)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZJV3ttBFlGsH"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "  def __init__(self, embed_size, sequence_length, head_size):\n",
        "    super(Head, self).__init__()\n",
        "    self.C = embed_size\n",
        "    self.L = sequence_length\n",
        "    self.head_size = head_size\n",
        "    self.q = nn.Linear(self.C, head_size, bias=False)\n",
        "    self.k = nn.Linear(self.C, head_size, bias=False)\n",
        "    self.v = nn.Linear(self.C, head_size, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    q = self.q(x)\n",
        "    k = self.k(x)\n",
        "    v = self.v(x)\n",
        "\n",
        "    wei = q @ k.transpose(-2, -1) * (self.head_size ** -0.5)\n",
        "    tril = torch.tril(torch.ones(self.L, self.L))\n",
        "    wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "    out = wei @ v\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eKQXoXVIx2Vd"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads, embedding_size, sequence_length, head_size):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "\n",
        "    self.heads = nn.ModuleList([\n",
        "        Head(embedding_size, sequence_length, head_size) for _ in range(num_heads)\n",
        "    ])\n",
        "\n",
        "    self.final_linear = nn.Linear(num_heads * head_size, embedding_size)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    head_outputs = [head(x) for head in self.heads]\n",
        "    concatenated_output = torch.cat(head_outputs, dim=-1)\n",
        "    final_output = self.relu(self.final_linear(concatenated_output))\n",
        "\n",
        "    return final_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "F7ZayT3ElZGQ"
      },
      "outputs": [],
      "source": [
        "class BlockAttention(nn.Module):\n",
        "  def __init__(self, num_heads, embedding_size, sequence_length, head_size):\n",
        "    super(BlockAttention, self).__init__()\n",
        "    self.multiheads = MultiHeadAttention(num_heads, embedding_size, sequence_length, head_size)\n",
        "    self.norm = nn.LayerNorm(embedding_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    inter_result = x + self.multiheads(x)\n",
        "    final_output = self.norm(x + inter_result)\n",
        "    return final_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WrLQtCOHnti1"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, num_heads, vocab_size, embedding_size, sequence_length, head_size):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.em = nn.Embedding(vocab_size, embedding_size)\n",
        "\n",
        "    self.blocks = nn.ModuleList([BlockAttention(num_heads, embedding_size, sequence_length, head_size) for _ in range(4)])\n",
        "    self.fw = nn.Linear(sequence_length * embedding_size, vocab_size, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.em(x)\n",
        "    for block in self.blocks:\n",
        "      x = block(x)\n",
        "    B,T,C = x.shape\n",
        "    x = x.view(B,1,T*C)\n",
        "    return self.fw(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "z5ONskhX-hQK"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "hidden_size = 100\n",
        "embedding_dim = 20\n",
        "seq_length = 8\n",
        "learning_rate = 1e-1\n",
        "batch_size = 20\n",
        "num_heads = 4\n",
        "head_size = 12\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "    #Decoder(num_heads, input_size, sequence_length, head_size)\n",
        "model = Decoder(num_heads, vocab_size, embedding_dim, seq_length, head_size)\n",
        "optimizer = optim.Adagrad(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "eH_L71Qo9i4v"
      },
      "outputs": [],
      "source": [
        "def generate_mini_batch():\n",
        "  # Assuming batch_size is a variable representing the desired batch size\n",
        "  # and data is your input sequence data\n",
        "\n",
        "  # Initialize lists to store input sequences and corresponding targets for the minibatch\n",
        "  batch_inputs = []\n",
        "  batch_targets = []\n",
        "\n",
        "  # Loop to generate the minibatch\n",
        "  for _ in range(batch_size):\n",
        "    # Randomly select a starting point for the sequence\n",
        "    p = np.random.randint(0, len(data) - seq_length - 1)\n",
        "\n",
        "    # Extract a sequence of characters and convert them to indices\n",
        "    inputs = torch.tensor([char_to_ix[ch] for ch in data[p:p + seq_length]], dtype=torch.long).view(1, -1)\n",
        "\n",
        "    # Extract the target character and convert it to an index\n",
        "    target = torch.tensor([char_to_ix[data[p + seq_length]]], dtype=torch.long).view(1, -1)\n",
        "\n",
        "    # Append the input sequence and target to the minibatch lists\n",
        "    batch_inputs.append(inputs)\n",
        "    batch_targets.append(target)\n",
        "\n",
        "  # Combine the lists into tensors to form the minibatch\n",
        "  minibatch_inputs = torch.cat(batch_inputs, dim=0)\n",
        "  minibatch_targets = torch.cat(batch_targets, dim=0)\n",
        "  return minibatch_inputs, minibatch_targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NRjINVa0-ZbW",
        "outputId": "321c9a62-f57d-447d-8d28-5a94c9405523"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Loss: 4.233497619628906\n",
            "Iteration 2000, Loss: 2.58974027633667\n",
            "Iteration 4000, Loss: 2.538651704788208\n",
            "Iteration 6000, Loss: 2.2114078998565674\n",
            "Iteration 8000, Loss: 2.224266290664673\n",
            "Iteration 10000, Loss: 2.657785415649414\n",
            "Iteration 12000, Loss: 2.408639907836914\n",
            "Iteration 14000, Loss: 2.0665955543518066\n",
            "Iteration 16000, Loss: 2.66404390335083\n",
            "Iteration 18000, Loss: 2.0059735774993896\n",
            "Iteration 20000, Loss: 2.588827133178711\n",
            "Iteration 22000, Loss: 1.6055160760879517\n",
            "Iteration 24000, Loss: 2.0929934978485107\n",
            "Iteration 26000, Loss: 2.200718402862549\n",
            "Iteration 28000, Loss: 2.5265440940856934\n",
            "Iteration 30000, Loss: 2.284790277481079\n",
            "Iteration 32000, Loss: 2.126494884490967\n",
            "Iteration 34000, Loss: 2.0061211585998535\n",
            "Iteration 36000, Loss: 2.3318209648132324\n",
            "Iteration 38000, Loss: 2.216878890991211\n",
            "Iteration 40000, Loss: 1.7851817607879639\n",
            "Iteration 42000, Loss: 1.7979450225830078\n",
            "Iteration 44000, Loss: 1.9491914510726929\n",
            "Iteration 46000, Loss: 2.2174718379974365\n",
            "Iteration 48000, Loss: 1.7806930541992188\n",
            "Iteration 50000, Loss: 1.772369623184204\n",
            "Iteration 52000, Loss: 2.0357065200805664\n",
            "Iteration 54000, Loss: 2.054630994796753\n",
            "Iteration 56000, Loss: 2.4326627254486084\n",
            "Iteration 58000, Loss: 2.021299362182617\n",
            "Iteration 60000, Loss: 2.2966246604919434\n",
            "Iteration 62000, Loss: 1.6522367000579834\n",
            "Iteration 64000, Loss: 1.7301967144012451\n",
            "Iteration 66000, Loss: 2.4130635261535645\n",
            "Iteration 68000, Loss: 1.875733733177185\n",
            "Iteration 70000, Loss: 2.0227723121643066\n",
            "Iteration 72000, Loss: 2.1468498706817627\n",
            "Iteration 74000, Loss: 1.5493788719177246\n",
            "Iteration 76000, Loss: 1.6512552499771118\n",
            "Iteration 78000, Loss: 1.9883472919464111\n",
            "Iteration 80000, Loss: 1.593249797821045\n",
            "Iteration 82000, Loss: 2.1149721145629883\n",
            "Iteration 84000, Loss: 2.0451407432556152\n",
            "Iteration 86000, Loss: 1.890815019607544\n",
            "Iteration 88000, Loss: 2.1348743438720703\n",
            "Iteration 90000, Loss: 2.0963776111602783\n",
            "Iteration 92000, Loss: 2.086240291595459\n",
            "Iteration 94000, Loss: 1.6294622421264648\n",
            "Iteration 96000, Loss: 2.0073790550231934\n",
            "Iteration 98000, Loss: 2.2831597328186035\n",
            "Iteration 100000, Loss: 1.8641889095306396\n",
            "Iteration 102000, Loss: 2.0380666255950928\n",
            "Iteration 104000, Loss: 2.416975975036621\n",
            "Iteration 106000, Loss: 2.5460333824157715\n",
            "Iteration 108000, Loss: 1.9705355167388916\n",
            "Iteration 110000, Loss: 1.7950624227523804\n",
            "Iteration 112000, Loss: 2.280472993850708\n",
            "Iteration 114000, Loss: 1.5416196584701538\n",
            "Iteration 116000, Loss: 2.163292646408081\n",
            "Iteration 118000, Loss: 2.663243293762207\n",
            "Iteration 120000, Loss: 1.7637618780136108\n",
            "Iteration 122000, Loss: 2.1500093936920166\n",
            "Iteration 124000, Loss: 1.8586307764053345\n",
            "Iteration 126000, Loss: 1.8862864971160889\n",
            "Iteration 128000, Loss: 2.1720705032348633\n",
            "Iteration 130000, Loss: 2.0301661491394043\n",
            "Iteration 132000, Loss: 2.155984401702881\n",
            "Iteration 134000, Loss: 1.9756205081939697\n",
            "Iteration 136000, Loss: 2.0373549461364746\n",
            "Iteration 138000, Loss: 2.0884718894958496\n",
            "Iteration 140000, Loss: 1.9197800159454346\n",
            "Iteration 142000, Loss: 2.5426177978515625\n",
            "Iteration 144000, Loss: 1.452934980392456\n",
            "Iteration 146000, Loss: 2.0404598712921143\n",
            "Iteration 148000, Loss: 1.1377689838409424\n",
            "Iteration 150000, Loss: 2.6471915245056152\n",
            "Iteration 152000, Loss: 1.4847956895828247\n",
            "Iteration 154000, Loss: 2.613588333129883\n",
            "Iteration 156000, Loss: 2.1048357486724854\n",
            "Iteration 158000, Loss: 1.576792597770691\n",
            "Iteration 160000, Loss: 1.5437549352645874\n",
            "Iteration 162000, Loss: 1.540468692779541\n",
            "Iteration 164000, Loss: 2.0044989585876465\n",
            "Iteration 166000, Loss: 1.445873498916626\n",
            "Iteration 168000, Loss: 2.0547423362731934\n",
            "Iteration 170000, Loss: 1.841395616531372\n",
            "Iteration 172000, Loss: 1.8388105630874634\n",
            "Iteration 174000, Loss: 1.4744054079055786\n",
            "Iteration 176000, Loss: 1.2823201417922974\n",
            "Iteration 178000, Loss: 1.6050059795379639\n",
            "Iteration 180000, Loss: 2.5615062713623047\n",
            "Iteration 182000, Loss: 1.7752046585083008\n",
            "Iteration 184000, Loss: 1.795175313949585\n",
            "Iteration 186000, Loss: 2.2898545265197754\n",
            "Iteration 188000, Loss: 1.967150330543518\n",
            "Iteration 190000, Loss: 2.2790236473083496\n",
            "Iteration 192000, Loss: 1.9278148412704468\n",
            "Iteration 194000, Loss: 2.186086654663086\n",
            "Iteration 196000, Loss: 2.1021337509155273\n",
            "Iteration 198000, Loss: 1.6083322763442993\n",
            "Iteration 200000, Loss: 1.658482313156128\n",
            "Iteration 202000, Loss: 1.4821841716766357\n",
            "Iteration 204000, Loss: 1.942858099937439\n",
            "Iteration 206000, Loss: 1.7427527904510498\n",
            "Iteration 208000, Loss: 1.3787637948989868\n",
            "Iteration 210000, Loss: 1.3975863456726074\n",
            "Iteration 212000, Loss: 1.9497582912445068\n",
            "Iteration 214000, Loss: 2.0940492153167725\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-e4d71cbf6d41>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m                             )\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adagrad.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mhas_sparse_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_sums\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             adagrad(\n\u001b[0m\u001b[1;32m    121\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adagrad.py\u001b[0m in \u001b[0;36madagrad\u001b[0;34m(params, grads, state_sums, state_steps, has_sparse_grad, foreach, differentiable, lr, weight_decay, lr_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adagrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    221\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adagrad.py\u001b[0m in \u001b[0;36m_single_tensor_adagrad\u001b[0;34m(params, grads, state_sums, state_steps, lr, weight_decay, lr_decay, eps, has_sparse_grad, maximize, differentiable)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# update step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mstep_t\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmaximize\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_stack_if_compiling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "stopi = []\n",
        "lossi = []\n",
        "num_iterations = 5\n",
        "for iteration in range(num_iterations):\n",
        "\n",
        "  for p in range(len(data) - seq_length):\n",
        "\n",
        "    # inputs = torch.tensor([char_to_ix[ch] for ch in data[p:p + seq_length]], dtype=torch.long).view(1, -1)\n",
        "    # targets = torch.tensor([char_to_ix[ch] for ch in data[p + seq_length]], dtype=torch.long).view(-1)\n",
        "\n",
        "    inputs, targets = generate_mini_batch()\n",
        "    optimizer.zero_grad()\n",
        "    predict_char = model(inputs)\n",
        "\n",
        "    loss = criterion(predict_char.view(-1, 65), targets.view(-1))\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    for param in model.parameters():\n",
        "      if param.grad is not None:\n",
        "        param.grad.data.clamp_(-5, 5)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    if p % 2000 == 0:\n",
        "      print(f'Iteration {(iteration + 1) * p}, Loss: {loss.item()}')\n",
        "      stopi.append((iteration + 1) * p)\n",
        "      lossi.append(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpgZG9toDSdb",
        "outputId": "12123b51-c229-4da0-aaba-17dcb826e397"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen, for mint re-thek wads astod is no thy mouspert to him the sugk astir enarm,\n",
            "He beceaw shallibs! I swall will with awitl dom bevest he'd Bustutter have grato.\n",
            "\n",
            "SucouLe mon or shee fitofrip, whe shere ctragey, a for a grotsting.\n",
            "Thy llsule lath\n",
            "Foun fomine hor thy piton bescino guist meridg to the the sall'd as,\n",
            "thnkvy, Benobed,\n",
            "Tis shid.\n",
            "\n",
            "RINA Bly:\n",
            "That seep word,\n",
            "That he were was a toonds! the wno mokipe some hey shyell: Ventranf amy to his in, Gifeseny in my hawce. Hatken'dl a my athen goodu.\n",
            "I Opllet, ware dory ar could; the and Reop and wut thou, I das he orsenmen:\n",
            "Coun sworkey soolt to plefornt; chap and but my, eremine thinks of mpady,\n",
            "Ware priglloues, wich wourd ast cueel on ait.\n",
            "\n",
            "KATHARINA:\n",
            "SirI, bead igain and triest,\n",
            "Gills pomy conjurrcns, ped herath, I case have nole:\n",
            "For het the courd would he did to bemay of her welary o sake withiwh moren\n",
            "Let take of me?\n",
            "\n",
            "MENEN:\n",
            "Wills Kason wreesp. O dI be,\n",
            "And thou herpf it!\n",
            "\n",
            "WANen:\n",
            "Ni seaim,\n",
            "Bod you fce on my thou thak fage;\n",
            "And attire\n"
          ]
        }
      ],
      "source": [
        "start = \"First Citizen\"\n",
        "\n",
        "for i in range(1000):\n",
        "  lll = start[-seq_length:]\n",
        "  ll = torch.tensor([char_to_ix[ch] for ch in lll], dtype=torch.long).view(1, -1)\n",
        "  outputs = model(ll)\n",
        "  p = nn.functional.softmax(outputs, dim=-1).detach().numpy().ravel()\n",
        "  ix = np.random.choice(range(vocab_size), p=p)\n",
        "  ix = torch.tensor(ix, dtype=torch.long).view(1, 1)\n",
        "  start += ix_to_char[ix[0][0].item()]\n",
        "\n",
        "print(start)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7AnsitoqLGRxdnb8G+GMb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}