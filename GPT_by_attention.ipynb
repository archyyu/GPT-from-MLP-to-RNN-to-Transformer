{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/archyyu/GPT-from-MLP-to-RNN-to-Transformer/blob/main/GPT_by_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ma80RtZJnLKK"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMBvKgnEpZmH",
        "outputId": "fc5302d5-6b0c-442e-cabc-e1c25fc3650e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data has 1115394 characters, 65 unique.\n"
          ]
        }
      ],
      "source": [
        "# Data I/O\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "response = requests.get(url)\n",
        "data = response.text\n",
        "\n",
        "chars = list(set(data))\n",
        "data_size, vocab_size = len(data), len(chars)\n",
        "print(f'data has {data_size} characters, {vocab_size} unique.')\n",
        "\n",
        "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
        "ix_to_char = {i: ch for i, ch in enumerate(chars)}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "embedding_dim = 64\n",
        "seq_length = 80\n",
        "learning_rate = 1e-1\n",
        "batch_size = 20\n",
        "num_heads = 4\n",
        "head_size = 32\n",
        "head_num = 4\n",
        "layer_num = 4\n",
        "dropout = 0.2"
      ],
      "metadata": {
        "id": "3w8BF25nr1dE"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "eKQXoXVIx2Vd"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "  def __init__(self, embed_size, head_size):\n",
        "    super(Head, self).__init__()\n",
        "    self.C = embed_size\n",
        "    self.head_size = head_size\n",
        "    self.q = nn.Linear(self.C, head_size, bias=False)\n",
        "    self.k = nn.Linear(self.C, head_size, bias=False)\n",
        "    self.v = nn.Linear(self.C, head_size, bias=False)\n",
        "\n",
        "    self.register_buffer('tril',torch.tril(torch.ones(seq_length, seq_length)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.shape\n",
        "    q = self.q(x)\n",
        "    k = self.k(x)\n",
        "    v = self.v(x)\n",
        "\n",
        "    wei = q @ k.transpose(-2, -1) * (self.head_size ** -0.5)\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "    out = wei @ v\n",
        "    return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads, embedding_size, head_size):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "\n",
        "    self.heads = nn.ModuleList([\n",
        "        Head(embedding_size, head_size) for _ in range(num_heads)\n",
        "    ])\n",
        "\n",
        "    self.final_linear = nn.Linear(num_heads * head_size, embedding_size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    head_outputs = [head(x) for head in self.heads]\n",
        "    concatenated_output = torch.cat(head_outputs, dim=-1)\n",
        "    final_output = self.final_linear(concatenated_output)\n",
        "    final_output = self.dropout(final_output)\n",
        "    return final_output\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "  def __init__(self, embedding_size):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "      nn.Linear(embedding_size, 4 * embedding_size),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(4 * embedding_size, embedding_size),\n",
        "      nn.Dropout(dropout),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "class BlockAttention(nn.Module):\n",
        "  def __init__(self, num_heads, embedding_size, head_size):\n",
        "    super(BlockAttention, self).__init__()\n",
        "    self.multiheads = MultiHeadAttention(num_heads, embedding_size, head_size)\n",
        "    self.fw = FeedFoward(embedding_size)\n",
        "    self.norm1 = nn.LayerNorm(embedding_size)\n",
        "    self.norm2 = nn.LayerNorm(embedding_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    inter_result = x + self.multiheads(self.norm1(x))\n",
        "    final_output = x + self.fw(self.norm2(inter_result))\n",
        "    return final_output\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self, num_heads, vocab_size, embedding_size, head_size):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.em = nn.Embedding(vocab_size, embedding_size)\n",
        "    self.pos_encode = nn.Embedding(seq_length, embedding_size)\n",
        "    self.blocks = nn.ModuleList([BlockAttention(num_heads, embedding_size, head_size) for _ in range(4)])\n",
        "    self.f_norm = nn.LayerNorm(embedding_size)\n",
        "    self.fw = nn.Linear(embedding_size, vocab_size, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T = x.shape\n",
        "    x_em = self.em(x)\n",
        "    p_em = self.pos_encode(torch.arange(T))\n",
        "    x = x_em + p_em\n",
        "    for block in self.blocks:\n",
        "      x = block(x)\n",
        "    x = self.f_norm(x)\n",
        "    x = self.fw(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "z5ONskhX-hQK"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "    #Decoder(num_heads, input_size, sequence_length, head_size)\n",
        "model = Decoder(num_heads, vocab_size, embedding_dim, head_size)\n",
        "optimizer = optim.Adagrad(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "eH_L71Qo9i4v"
      },
      "outputs": [],
      "source": [
        "def generate_mini_batch():\n",
        "  # Assuming batch_size is a variable representing the desired batch size\n",
        "  # and data is your input sequence data\n",
        "\n",
        "  # Initialize lists to store input sequences and corresponding targets for the minibatch\n",
        "  batch_inputs = []\n",
        "  batch_targets = []\n",
        "\n",
        "  seq_len = seq_length #torch.randint(low=1, high=seq_length + 1, size=(1,)).item()\n",
        "\n",
        "  # Loop to generate the minibatch\n",
        "  for _ in range(batch_size):\n",
        "    # Randomly select a starting point for the sequence\n",
        "    p = np.random.randint(0, len(data) - seq_len - 1)\n",
        "\n",
        "    # Extract a sequence of characters and convert them to indices\n",
        "    inputs = torch.tensor([char_to_ix[ch] for ch in data[p:p+seq_len]], dtype=torch.long).view(1, -1)\n",
        "\n",
        "    # Extract the target character and convert it to an index\n",
        "    target = torch.tensor([char_to_ix[ch] for ch in data[p+1:p+seq_len+1]], dtype=torch.long).view(1, -1)\n",
        "\n",
        "    # Append the input sequence and target to the minibatch lists\n",
        "    batch_inputs.append(inputs)\n",
        "    batch_targets.append(target)\n",
        "\n",
        "  # Combine the lists into tensors to form the minibatch\n",
        "  minibatch_inputs = torch.cat(batch_inputs, dim=0)\n",
        "  minibatch_targets = torch.cat(batch_targets, dim=0)\n",
        "  return minibatch_inputs, minibatch_targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRjINVa0-ZbW"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "stopi = []\n",
        "lossi = []\n",
        "num_iterations = 5\n",
        "for iteration in range(num_iterations):\n",
        "\n",
        "  for p in range(len(data) - seq_length):\n",
        "\n",
        "    # inputs = torch.tensor([char_to_ix[ch] for ch in data[p:p + seq_length]], dtype=torch.long).view(1, -1)\n",
        "    # targets = torch.tensor([char_to_ix[ch] for ch in data[p + seq_length]], dtype=torch.long).view(-1)\n",
        "\n",
        "    inputs, targets = generate_mini_batch()\n",
        "    optimizer.zero_grad()\n",
        "    predict_char = model(inputs)\n",
        "\n",
        "    B, T = inputs.shape\n",
        "\n",
        "    logits = predict_char.view(B*T, -1)\n",
        "    targets = targets.view(B*T)\n",
        "\n",
        "    loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    for param in model.parameters():\n",
        "      if param.grad is not None:\n",
        "        param.grad.data.clamp_(-5, 5)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    if p % 2000 == 0:\n",
        "      print(f'Iteration {(iteration + 1) * p}, Loss: {loss.item()}')\n",
        "      stopi.append((iteration + 1) * p)\n",
        "      lossi.append(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpgZG9toDSdb",
        "outputId": "0483ec32-bf80-4fcc-833b-6c3709f97ae9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen: by very so win to Jerived.\n",
            "Cominist make thou, forth it Rings:\n",
            "Alive and conce; telike you bame of and moth.\n",
            "\n",
            "HENROLINGBUBE:\n",
            "Ay, Go, see, and my land to in therir serve.\n",
            "\n",
            "JULIET:\n",
            "Tell the my brothy flooze prantle; what that see hourse\n",
            "Hold the stan deen I am no and soe may me the me.\n",
            "\n",
            "Secock:\n",
            "Edw thin she agaid it peacant be cand wovy.\n",
            "tress, Beickeifes I'll reath theem but I twas are.\n",
            "\n",
            "ISTRALANUS:\n",
            "Thou of see nevoing heart\n",
            "Thy tights your congrest on he and trust;\n",
            "To mand the I fitisped is my conter,\n",
            "Prepsoring vientit of server their take\n",
            "The graces-dies by tile all sondeet must\n",
            "EvOr somine bithers nobhy emy reventurous,\n",
            "I scandl hourst see, if this So fone.\n",
            "And I, do but on that fath own the he will.\n",
            "\n",
            "KING RICHARD III:\n",
            "\n",
            "Your Thus no as for pervoishe,\n",
            "What rett to his shis fa you quencion\n",
            "Thaver, sich on you risuman'd a 'stimage.\n",
            "\n",
            "CAMILO:\n",
            "Well, terviged:\n",
            "Ay, I no pelsets, the good are air schil.\n",
            "\n",
            "RICHAS:\n",
            "Ay, make for our find slord he,\n",
            "Let brikest of him.\n",
            "\n",
            "Werst Yor the their me gi\n"
          ]
        }
      ],
      "source": [
        "start = \"First Citizen\"\n",
        "\n",
        "for i in range(1000):\n",
        "  lll = start[-seq_length:]\n",
        "  ll = torch.tensor([char_to_ix[ch] for ch in lll], dtype=torch.long).view(1, -1)\n",
        "  outputs = model(ll)\n",
        "  outputs = torch.squeeze(outputs)\n",
        "  outputs = outputs[-1,:]\n",
        "  p = nn.functional.softmax(outputs, dim=-1).detach().numpy().ravel()\n",
        "  ix = np.random.choice(range(vocab_size), p=p)\n",
        "  ix = torch.tensor(ix, dtype=torch.long).view(1, 1)\n",
        "  start += ix_to_char[ix[0][0].item()]\n",
        "\n",
        "print(start)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "after some modification, this version of the Decoder is better now.\n",
        "\n",
        "but I still has some issues on that.\n",
        "\n",
        "anyway, let us stop in here"
      ],
      "metadata": {
        "id": "tPkdQ3mQA0Rr"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPo6afaFTZC+nhKwKAmonx8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}