{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMmbgfxVoqQUUF0X8xFfzt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/archyyu/RNN-GPT/blob/main/StudyGRU1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4bRtSwiVsdU",
        "outputId": "9f5e8082-72a9-45bf-9313-c45eaf257fb2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x78640c780110>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data I/O\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "#url = \"https://raw.githubusercontent.com/archyyu/publicResource/main/google.dev.en\"\n",
        "#url = \"https://raw.githubusercontent.com/tinygrad/tinygrad/master/tinygrad/tensor.py\"\n",
        "#url = \"https://raw.githubusercontent.com/archyyu/publicResource/main/KDE4.en-es.en\"\n",
        "#url = \"https://raw.githubusercontent.com/archyyu/publicResource/main/js\"\n",
        "response = requests.get(url)\n",
        "data = response.text\n",
        "\n",
        "chars = list(set(data))\n",
        "data_size, vocab_size = len(data), len(chars)\n",
        "print(f'data has {data_size} characters, {vocab_size} unique.')\n",
        "\n",
        "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
        "ix_to_char = {i: ch for i, ch in enumerate(chars)}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7T7UgVY5VweN",
        "outputId": "f396d441-b000-497e-8c7c-6a34c62a1aee"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data has 1115394 characters, 65 unique.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "hidden_size = 128\n",
        "embedding_dim = 64\n",
        "seq_length = 30\n",
        "learning_rate = 0.01\n",
        "batch_size = 20"
      ],
      "metadata": {
        "id": "E-MaNEStVxxQ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class GRUCell(nn.Module):\n",
        "  def __init__(self, input_size, embedding_dim, hidden_size, output_size):\n",
        "    super(GRUCell, self).__init__()\n",
        "    self.embedding = nn.Embedding(input_size, embedding_dim)\n",
        "    self.Wr = nn.Linear(embedding_dim, hidden_size, bias=True)\n",
        "    self.Hr = nn.Linear(hidden_size, hidden_size,bias=True)\n",
        "    self.Wz = nn.Linear(embedding_dim, hidden_size, bias=True)\n",
        "    self.Hz = nn.Linear(hidden_size, hidden_size,bias=True)\n",
        "    self.Wh = nn.Linear(embedding_dim, hidden_size, bias=True)\n",
        "    self.Hh = nn.Linear(hidden_size, hidden_size,bias=True)\n",
        "    self.rb = nn.Parameter(torch.zeros(1, hidden_size))\n",
        "    self.zb = nn.Parameter(torch.zeros(1, hidden_size))\n",
        "    self.hb = nn.Parameter(torch.zeros(1, hidden_size))\n",
        "    self.Ho = nn.Linear(hidden_size, output_size)\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self):\n",
        "    for layer in [self.Wr, self.Hr, self.Wz, self.Hz, self.Wh, self.Hh]:\n",
        "      nn.init.xavier_uniform_(layer.weight.data)\n",
        "\n",
        "  def forward(self, x, h_prev):\n",
        "    x = self.embedding(x)\n",
        "    rt = torch.sigmoid(self.Wr(x) + self.Hr(h_prev) + self.rb)\n",
        "    zt = torch.sigmoid(self.Wz(x) + self.Hz(h_prev) + self.zb)\n",
        "\n",
        "    tht = torch.tanh(self.Wh(x) + rt * self.Hh(h_prev) + self.hb)\n",
        "    hz = zt * tht + (1 - zt) * h_prev\n",
        "    y = self.Ho(hz)\n",
        "    return y, hz\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model = GRUCell(vocab_size, embedding_dim, hidden_size, vocab_size)\n",
        "optimizer = optim.Adagrad(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "Ck8Yr4rcZ2QV"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generateMiniBatch(start_idx):\n",
        "  batch_inputs = []\n",
        "  batch_targets = []\n",
        "\n",
        "  # Generate examples for the current minibatch\n",
        "  for i in range(batch_size):\n",
        "    p = start_idx + i\n",
        "    inputs = torch.tensor([char_to_ix[ch] for ch in data[p:p + seq_length]], dtype=torch.long).view(1, -1)\n",
        "    targets = torch.tensor([char_to_ix[ch] for ch in data[p + 1:p + seq_length + 1]], dtype=torch.long).view(-1)\n",
        "\n",
        "    batch_inputs.append(inputs)\n",
        "    batch_targets.append(targets)\n",
        "\n",
        "  # Convert lists to tensors\n",
        "  minibatch_inputs = torch.cat(batch_inputs, dim=0)\n",
        "  minibatch_targets = torch.stack(batch_targets)\n",
        "  return minibatch_inputs, minibatch_targets"
      ],
      "metadata": {
        "id": "6KPju9mMNict"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "import sys\n",
        "stopi = []\n",
        "lossi = []\n",
        "num_iterations = 10000\n",
        "p = 0\n",
        "for iteration in range(num_iterations):\n",
        "\n",
        "  if p + seq_length + 1 > len(data):\n",
        "    p = 0;\n",
        "\n",
        "  # inputs = torch.tensor([char_to_ix[ch] for ch in data[p:p + seq_length]], dtype=torch.long).view(1, -1)\n",
        "  # targets = torch.tensor([char_to_ix[ch] for ch in data[p + 1:p + seq_length + 1]], dtype=torch.long).view(-1)\n",
        "\n",
        "  inputs, targets = generateMiniBatch(p)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  totalloss = 0\n",
        "  hprev = torch.zeros((1, 1, hidden_size))  # Reset RNN memory\n",
        "  for i in range(seq_length):\n",
        "    input_char = inputs[:,i].unsqueeze(1)\n",
        "    output_char = targets[:,i]\n",
        "\n",
        "    output, hprev = model(input_char, hprev)\n",
        "    loss = criterion(output.squeeze(1), output_char)\n",
        "    totalloss += loss.item()\n",
        "    loss.backward()\n",
        "    hprev = hprev.detach()\n",
        "\n",
        "    for param in model.parameters():\n",
        "      if param.grad is not None:\n",
        "        param.grad.data.clamp_(-5, 5)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "  if iteration % 100 == 0:\n",
        "    print(f'Iteration {iteration}, Loss: {totalloss/seq_length}')\n",
        "    stopi.append(iteration)\n",
        "    lossi.append(totalloss/seq_length)\n",
        "\n",
        "  p += seq_length  # Move data pointer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "65XePBmxV0Ze",
        "outputId": "222006a3-ec58-41d5-87ea-0337720a170f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Loss: 1.4610637605190278\n",
            "Iteration 100, Loss: 2.3381783803304037\n",
            "Iteration 200, Loss: 1.8641392668088277\n",
            "Iteration 300, Loss: 2.264891723791758\n",
            "Iteration 400, Loss: 2.351759696006775\n",
            "Iteration 500, Loss: 2.152710155646006\n",
            "Iteration 600, Loss: 2.0079333662986754\n",
            "Iteration 700, Loss: 1.784627914428711\n",
            "Iteration 800, Loss: 1.9996415297190349\n",
            "Iteration 900, Loss: 1.9682504216829935\n",
            "Iteration 1000, Loss: 1.9921839833259583\n",
            "Iteration 1100, Loss: 1.6524122436841329\n",
            "Iteration 1200, Loss: 1.9321902592976887\n",
            "Iteration 1300, Loss: 1.5889442245165506\n",
            "Iteration 1400, Loss: 2.451417334874471\n",
            "Iteration 1500, Loss: 1.7988687435785928\n",
            "Iteration 1600, Loss: 1.9366730332374573\n",
            "Iteration 1700, Loss: 1.7445647478103639\n",
            "Iteration 1800, Loss: 1.1422361791133882\n",
            "Iteration 1900, Loss: 1.9508732696374258\n",
            "Iteration 2000, Loss: 2.3604846477508543\n",
            "Iteration 2100, Loss: 1.7072551568349204\n",
            "Iteration 2200, Loss: 2.049198559919993\n",
            "Iteration 2300, Loss: 1.2621296107769013\n",
            "Iteration 2400, Loss: 1.857106912136078\n",
            "Iteration 2500, Loss: 1.6376813928286234\n",
            "Iteration 2600, Loss: 1.610942639907201\n",
            "Iteration 2700, Loss: 1.6303119619687398\n",
            "Iteration 2800, Loss: 1.8767916321754456\n",
            "Iteration 2900, Loss: 1.9459256211916605\n",
            "Iteration 3000, Loss: 1.8330224831899007\n",
            "Iteration 3100, Loss: 1.8621703346570333\n",
            "Iteration 3200, Loss: 2.127026096979777\n",
            "Iteration 3300, Loss: 1.3032311538855235\n",
            "Iteration 3400, Loss: 1.7181057612101236\n",
            "Iteration 3500, Loss: 1.8359437227249145\n",
            "Iteration 3600, Loss: 2.5209995985031126\n",
            "Iteration 3700, Loss: 1.2578043778737387\n",
            "Iteration 3800, Loss: 1.8819470763206483\n",
            "Iteration 3900, Loss: 1.8489613294601441\n",
            "Iteration 4000, Loss: 1.7875566283861797\n",
            "Iteration 4100, Loss: 2.0023163040479024\n",
            "Iteration 4200, Loss: 1.957479759057363\n",
            "Iteration 4300, Loss: 1.758107070128123\n",
            "Iteration 4400, Loss: 2.0286118904749553\n",
            "Iteration 4500, Loss: 2.0453352451324465\n",
            "Iteration 4600, Loss: 1.1706205169359842\n",
            "Iteration 4700, Loss: 1.9274619102478028\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-e3663a9e311c>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_char\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mtotalloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mhprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhprev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample from the model\n",
        "def sample(model, seed_ix, n):\n",
        "  h = torch.zeros((1, 1, hidden_size))\n",
        "  x = torch.tensor(seed_ix, dtype=torch.long).view(1, 1)\n",
        "  ixes = []\n",
        "\n",
        "  for _ in range(n):\n",
        "    o, h = model(x, h)\n",
        "    p = nn.functional.softmax(o, dim=-1).detach().numpy().ravel()\n",
        "    ix = np.random.choice(range(vocab_size), p=p)\n",
        "    x = torch.tensor(ix, dtype=torch.long).view(1, 1)\n",
        "    ixes.append(ix)\n",
        "\n",
        "  return ixes"
      ],
      "metadata": {
        "id": "tjArcmrDV09t"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate sample text\n",
        "sample_ix = sample(model, char_to_ix[data[0]], 2000)\n",
        "txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
        "print(f'Generated Text:\\n{txt}')"
      ],
      "metadata": {
        "id": "abJam_S7V3Y8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0e6b6fa-50fe-4735-9736-fa0c7dab6b66"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            "a?\n",
            "Thans? Thow, Thowbur thou of Dourtul thelome me hive them to lemply netin,\n",
            "This t, word o' is cou; to your not to crest's the hink stote wosold shall for your? Oue have thy fore them it hal.\n",
            "\n",
            "First Sough eque; I'll waiss\n",
            "Ar hose spak leter.\n",
            "Wevery is in wheirny I hou:\n",
            "Thinch corned the shoughter\n",
            "Ford; we a arling, therears chinnot,\n",
            "We we pase may not\n",
            "Coit\n",
            "'The serifed, so would not. so shal'd sunstly 't sunter than then he the fromace, oble wempt\n",
            "Whan not hem? goich requfor,\n",
            "Whow mere,\n",
            "We chount, our chce to did for wore thibugh given ally grod;\n",
            "Wit not\n",
            "Cot bare whomith thust the parce.' shall for novest to unce a mister the's requerved on fere and poove, he neversy rodsise is choust's relsciery,\n",
            "Whosthonst sthou and be's a pourcing confitiants,\n",
            "I shall if? hith consuchate\n",
            "Thaid the shis wit to hirmwer Rome, monest to moest;\n",
            "That ins.\n",
            "\n",
            "VOLUMNIOLANUS:\n",
            "Grod the fives come leave morise breal;''Wson\n",
            "show\n",
            "Ro leak, of think, enound\n",
            "Thater to say spe'ty do with on that chou the nauk, with in cupe!\n",
            "The when; stack.\n",
            "\n",
            "VOLUMNIA:\n",
            "The cream.'\n",
            "To thate one mile,\n",
            "When\n",
            "Which wass suptening; und vers course thy bete on the Dity you brop holdinit; the worth,\n",
            "Whe's in thought cobless and to no my hath youramen, you, con our deing the not morthed pore hather reque ware nor,\n",
            "ve,\n",
            "Thonoust In thein!\n",
            "Cou for wiws soothers will the en by my to lasurst mnam it have for our cuppe your is anstry,\n",
            "Lill.\n",
            "\n",
            "Thade,\n",
            "What's\n",
            "The cyry, kno\n",
            "Your yould buth'ssel, in seaknet me do nove,\n",
            "And whughture must nairitch, prays not them achour as nor say tore,\n",
            "VOLUMNIA:\n",
            "Whate, but what on thove's ass that's on, coul coun's timebyI?\n",
            "You him;\n",
            "Romal,\n",
            "As wasponsins,\n",
            "That, sair wiuld to my to shen the no of priman way wown did so\n",
            "I an up wetered some, i' hid it we pteme:\n",
            "Thike them poned, lrobuner esinor kleqOrantess there'd\n",
            "Withers,\n",
            "And mae me sow endey! the say be the he bard ene the en wislety sing hose uptore, hould therers., and 'troud,\n",
            "Bes. Prequs. us thy ffor'dens nore uprid,\n",
            "And not no more\n",
            "Tome dond,\n",
            "Are \n"
          ]
        }
      ]
    }
  ]
}