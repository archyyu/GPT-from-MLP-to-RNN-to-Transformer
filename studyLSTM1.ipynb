{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyLZFp91Fwkw6AQYsm91pf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/archyyu/GPT-from-MLP-to-RNN-to-Transformer/blob/main/studyLSTM1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Od1zoq-gONwv",
        "outputId": "73af38cd-0e0f-4128-87b4-7e6dac4b6b37"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7e13544b8330>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data I/O\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "#url = \"https://raw.githubusercontent.com/archyyu/publicResource/main/google.dev.en\"\n",
        "#url = \"https://raw.githubusercontent.com/tinygrad/tinygrad/master/tinygrad/tensor.py\"\n",
        "#url = \"https://raw.githubusercontent.com/archyyu/publicResource/main/KDE4.en-es.en\"\n",
        "#url = \"https://raw.githubusercontent.com/archyyu/publicResource/main/js\"\n",
        "response = requests.get(url)\n",
        "data = response.text\n",
        "\n",
        "chars = list(set(data))\n",
        "data_size, vocab_size = len(data), len(chars)\n",
        "print(f'data has {data_size} characters, {vocab_size} unique.')\n",
        "\n",
        "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
        "ix_to_char = {i: ch for i, ch in enumerate(chars)}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Pyxhu9pOWln",
        "outputId": "4727ab0b-8bba-4a01-f2d7-2b34252fe515"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data has 1115394 characters, 65 unique.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "hidden_size = 100\n",
        "embedding_dim = 50\n",
        "seq_length = 25\n",
        "learning_rate = 0.0005\n",
        "batch_size = 20"
      ],
      "metadata": {
        "id": "mWSLiGv2OX8L"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LSTMCell(nn.Module):\n",
        "  def __init__(self, input_size, embedding_size, hidden_size):\n",
        "    super(LSTMCell, self).__init__()\n",
        "    self.emb = nn.Embedding(input_size, embedding_size)\n",
        "    self.W_f = nn.Linear(embedding_size, hidden_size, bias=False)\n",
        "    self.h_f = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "    self.b_f = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "    # Parameters for Input Gate\n",
        "    self.W_i = nn.Linear(embedding_size, hidden_size, bias=False)\n",
        "    self.h_i = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "    self.b_i = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "    # Parameters for Candidate Cell State\n",
        "    self.W_C = nn.Linear(embedding_size, hidden_size, bias=False)\n",
        "    self.h_C = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "    self.b_C = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "    # Parameters for Output Gate\n",
        "    self.W_o = nn.Linear(embedding_size, hidden_size, bias=False)\n",
        "    self.h_o = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "    self.b_o = nn.Parameter(torch.zeros(hidden_size))\n",
        "\n",
        "    self.t_o = nn.Linear(hidden_size, input_size, bias=False)\n",
        "    self.o_b = nn.Parameter(torch.zeros(input_size))\n",
        "\n",
        "  def forward(self, x, h, C_prev):\n",
        "    x = self.emb(x)\n",
        "    f_t = torch.sigmoid(self.W_f(x) + self.h_f(h) + self.b_f)\n",
        "    i_t = torch.sigmoid(self.W_i(x) + self.h_i(h) + self.b_i)\n",
        "    C_tilde = torch.tanh(self.W_C(x) + self.h_C(h) + self.b_C)\n",
        "    C_t = f_t * C_prev + i_t * C_tilde\n",
        "    o_t = torch.sigmoid(self.W_o(x) + self.h_o(h) + self.b_o)\n",
        "    h_t = o_t * torch.tanh(C_t)\n",
        "\n",
        "    output = self.t_o(h_t) + self.o_b\n",
        "\n",
        "    return output, h_t, C_t\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Model initialization\n",
        "model = LSTMCell(vocab_size, embedding_dim, hidden_size)\n",
        "optimizer = optim.Adagrad(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "E_5CVr65OYmY"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generateMiniBatch(start_idx):\n",
        "  batch_inputs = []\n",
        "  batch_targets = []\n",
        "\n",
        "  # Generate examples for the current minibatch\n",
        "  for i in range(batch_size):\n",
        "    p = start_idx + i\n",
        "    inputs = torch.tensor([char_to_ix[ch] for ch in data[p:p + seq_length]], dtype=torch.long).view(1, -1)\n",
        "    targets = torch.tensor([char_to_ix[ch] for ch in data[p + 1:p + seq_length + 1]], dtype=torch.long).view(-1)\n",
        "\n",
        "    batch_inputs.append(inputs)\n",
        "    batch_targets.append(targets)\n",
        "\n",
        "  # Convert lists to tensors\n",
        "  minibatch_inputs = torch.cat(batch_inputs, dim=0)\n",
        "  minibatch_targets = torch.stack(batch_targets)\n",
        "  return minibatch_inputs, minibatch_targets"
      ],
      "metadata": {
        "id": "3aWNU6tYdyZV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopi = []\n",
        "lossi = []"
      ],
      "metadata": {
        "id": "NukGmxejd0g2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_iterations = 10000\n",
        "p = 0\n",
        "for iteration in range(num_iterations):\n",
        "\n",
        "  if p + seq_length + 1 > len(data):\n",
        "    p = 0;\n",
        "\n",
        "  inputs, targets = generateMiniBatch(p)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  totalloss = 0\n",
        "  hprev = torch.zeros(1, hidden_size)\n",
        "  cprev = torch.zeros(1, hidden_size)\n",
        "  for i in range(seq_length):\n",
        "    input_char = inputs[:,i].unsqueeze(1)\n",
        "    output_char = targets[:,i]\n",
        "\n",
        "    predict_char, hprev, cprev = model(input_char, hprev, cprev)\n",
        "\n",
        "    loss = criterion(predict_char.squeeze(1), output_char)\n",
        "    totalloss += loss.item()\n",
        "\n",
        "    loss.backward()\n",
        "    hprev = hprev.detach()\n",
        "    cprev = cprev.detach()\n",
        "\n",
        "    for param in model.parameters():\n",
        "      if param.grad is not None:\n",
        "        param.grad.data.clamp_(-5, 5)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "  if iteration % 1000 == 0:\n",
        "    print(f'Iteration {iteration}, Loss: {totalloss/seq_length}')\n",
        "    stopi.append(iteration)\n",
        "    lossi.append(totalloss/seq_length)\n",
        "\n",
        "  p += seq_length  # Move data pointer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IiHebrWd23f",
        "outputId": "6f6dcb28-d224-465f-a2ec-ab63363d5e3e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Loss: 4.077442092895508\n",
            "Iteration 1000, Loss: 2.5910097217559813\n",
            "Iteration 2000, Loss: 2.535662069320679\n",
            "Iteration 3000, Loss: 3.018662900924683\n",
            "Iteration 4000, Loss: 2.9288361358642576\n",
            "Iteration 5000, Loss: 2.444118938446045\n",
            "Iteration 6000, Loss: 2.5324393367767333\n",
            "Iteration 7000, Loss: 2.1878686141967774\n",
            "Iteration 8000, Loss: 2.3422654914855956\n",
            "Iteration 9000, Loss: 2.186244306564331\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample from the model\n",
        "def sample(model, seed_ix, n):\n",
        "  h = torch.zeros(1, hidden_size)\n",
        "  c = torch.zeros(1, hidden_size)\n",
        "  x = torch.tensor(seed_ix, dtype=torch.long).view(1, 1)\n",
        "  ixes = []\n",
        "\n",
        "  for _ in range(n):\n",
        "    outputs, h, c = model(x, h, c)\n",
        "    # p = nn.functional.softmax(outputs, dim=-1)\n",
        "    # ix = torch.argmax(p).item()\n",
        "    p = nn.functional.softmax(outputs, dim=-1).detach().numpy().ravel()\n",
        "    ix = np.random.choice(range(vocab_size), p=p)\n",
        "    x = torch.tensor(ix, dtype=torch.long).view(1, 1)\n",
        "\n",
        "    if ix_to_char[ix] == '\\n':\n",
        "      print(''.join([ix_to_char[i] for i in ixes]))\n",
        "      ixes = []\n",
        "      continue\n",
        "\n",
        "    ixes.append(ix)\n",
        "\n",
        "  return ixes\n",
        "\n",
        "sample(model, char_to_ix[data[0]], 2000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zk_kICad5Y1",
        "outputId": "dfe773d1-3135-4138-8a41-0a5de19e4026"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "os neth,\n",
            "Moarl mf prit adfl sou\n",
            "Mausd,\n",
            "Wvhe t.lo.\n",
            "Ner thered hoer mray fm.\n",
            "Weinpad litut ur ne yot.\n",
            "\n",
            "QUve that tal lfeitpis menw,\n",
            "Sour nomyo hat se!s.\n",
            "ganutor whe no I, id tis, dive ow o hams lpuilgot borfaiidir uod par.\n",
            "\n",
            "\n",
            "T\n",
            "YoMnbeas, Pinbstes anth ham tineg:\n",
            "Sift aurd me atir, ball, tcye, beh me gal, den ,n gour ors micwot've!\n",
            "se v whe tor thir: en,\n",
            "Hen nod gy uopott fel mahis y yylt youw arbey, gicevcyd Kullitpt watheczns od han ay nodest oud path, agbertheaitd the w' wease:\n",
            "Wheatel heme sen derce heay thalde bes pembGDfhe haced,,\n",
            "Ct a d ade\n",
            "T; awerer:\n",
            "ay the fon Iol dwithe boorsiche irs fos the 'rt wath rstinortsdld s bnake, Ior gold hrthiowud, ste;:\n",
            "NpOsAarsle vam.\n",
            "Eihome yent yathet a,\n",
            "As dilr Hans we Iey vers have s. lolacrSd dat  Lot ann\n",
            " ne oal mons,\n",
            "\n",
            "Yam'Yor.\n",
            "Toond Cllt woth hlort ae ncrpkie'to aAe\n",
            "bin lyent MarCs thet us tuathigs.\n",
            "TerelYlre, fouelsegl'dn,\n",
            "CsAv, of woky I avet MirwiesI I ild Roat lart mand wal thom ou ,be wasruHrster woms my hawstn ouand Mel'vakg fit rshonce lloatn yn ong enotad Oed yitueirt me kamimUacn bavety, r dof hent fe; wf nao yor soiy.\n",
            "Lp ereby incyt ay aln thadef the forade hanvme this ursohe, ard hatt,\n",
            "Ss that homof mo,\n",
            "\n",
            "I,\n",
            "Tho'deere\n",
            "Thided? goln malf wove.\n",
            "G\n",
            "Aele.\n",
            "I hamenseatenth !weot ose ht na nr stlpn non oferent t\n",
            "yot bale thops mapco du pocnth fheing huosteallde oany ssUolH bind fotwbt selre all mir nor yoandr hotr?n w, the yormen hege hay?\n",
            "TI I  yo Cisiture noe cagougl may sitip mati?\n",
            "Bhnd rillegocen?Dw,\n",
            "Yooourn yyoreor math rau Oe welt to thlent siru gen the cuy pllad tat dit a ge simalhe; wimels butherte 'resCties seyace wheaCe cithes:\n",
            "I theen,\n",
            "The ast beorg\n",
            "msh tre wels 'erstod godcdde\n",
            "hfou he ither hinxs, Ret atery a.\n",
            "I\n",
            "\n",
            "OHRNawed nmd, las tint edot peuOkleoer, g noed fhean'angck leraen wenth r tow urd wiinAwENhe arwe they sugn\n",
            "Tou tou, ane geurs be maed; Hede fit she holay yorr inkthes wins.\n",
            "\n",
            "\n",
            "Snahit yoor tofr heart pour r'rikinkdsk mod, aanbe w too f fhs, loy cers hatofd wer''ler nod, of mat fourt,t beifle he.\n",
            "pone;:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample(model, char_to_ix[data[0]], 2000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZruqH9O5fNY",
        "outputId": "39a5eea5-f493-426d-db51-8d1df8f576e0"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "him I onendt yoce; wed bexwherHI thaw Tmell oome fanrd aed de bes comender ber all cev aTse.\n",
            "\n",
            "Iod sef, nouee' whir kos t key plo the, malar,\n",
            "Of witel\n",
            "Thime,t per t ohpt mtiVey\n",
            "Ocer gient gthe b by. herh soreae thig ceoeesth it.\n",
            "Ant olr, therce lir\n",
            "Whpini\n",
            "Horen\n",
            "dn ne daosl un the doetsr.\n",
            "I fos old wourcd an milt yas, time mesiFsierwe therednwanche tmd, , what aoercem that hit misr'lde cois andse pent arld nrss, yaver; oumearo they pis theenge ayr hoave tfse ind s h tot for wer gerst p hathut'Ht wine releecin'ge isfd oHd kin, marem suEecord nondh oringal,\n",
            "Whe anTin;M:hes,\n",
            "the urrid hekelin' the ance, ghe ives hut wtorsetit matiravutgs 'grt tha mi,\n",
            "SDsl ven hat my so thd'yy ou hit bime ond mir!y sas Ior\n",
            "SALFIIU!foOfE   hlOd, hid, m-uy chers-e nocrd,  be ter,\n",
            "Abe bmathe do yur -os wer, osheneat leakt hupensidhe ve de fothu thos eere,t ay bollathu pred alef foime thalshe nhltser,\n",
            "Mce tothe fw themel d rome, f icau am fir, wall mondt he thainhel thit oart hag loow Irel, can v ethem lade ort;, ins?\n",
            "Hvos honm:\n",
            "\n",
            "CIFey foowd iciter.\n",
            "Thinpenl .nd thir tath hovtorefusie' sder, gan\n",
            "HEE-ind pree lee t, you aid. hegr.\n",
            "\n",
            "Houcealin hn\n",
            ":\n",
            "Bels Wre\n",
            "Tnpir owed monksCtk!\n",
            "fullnt, fo:d youbpa hi this\n",
            "T;aen, hilss he tins\n",
            "Wy.\n",
            "\n",
            "FOf mqedss he toow\n",
            "Agoth and buulis pe too an b md mat lale wavakis ige that sotr buld ind,\n",
            "Thiag:\n",
            "Tuer tis thae.\n",
            "Abef?\n",
            "Tehalis manceefry Iteal anmind mith weN ours,\n",
            "\n",
            "Bgese arein, ygr-\n",
            "wan qy mibisdarder hs uroth iood grst anf lot foor phanga, toiq deschitmon omices too dno'd morend bl oot horejome I aly ores insly kam le dote domayU\n",
            "Te?rs.\n",
            "E.Bowk herer oom, f wor thliend ed oedy hat iungsyO hat wat wakd, ceoudele adhe,ncue wphenn guricl ovewe fo Ailo fhe ciuellat hf vre wiseg ber;u nurd\n",
            "Way sth cay prs the airs whe esat\n",
            "\n",
            "wam hiw alt dy oforumand, yaky wha, the she mind tay th to n pr.\n",
            "\n",
            "Bto dellld\n",
            "Hsame; Ald sinde nean teanelce it os oofeM\n",
            "Ake inonthpr be lean hacan rll ukalnl we cagil illd!\n",
            "\n",
            "Sre cath Mes, toa anre athae os &-ue Iole thede yeng''d lond,\n",
            "LEer fore rour\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    }
  ]
}