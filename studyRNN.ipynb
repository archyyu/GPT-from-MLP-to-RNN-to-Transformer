{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmaRVxPe/dEku9E+2+O2bP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/archyyu/RNN-GPT/blob/main/studyRnn4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "id": "uOcLhTikoHXs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7f98358-4cfb-4783-e1d5-633d63728f0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f43301b8130>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data I/O\n",
        "\n",
        "#url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "#url = \"https://raw.githubusercontent.com/archyyu/publicResource/main/google.dev.en\"\n",
        "url = \"https://raw.githubusercontent.com/torvalds/linux/master/mm/madvise.c\"\n",
        "response = requests.get(url)\n",
        "data = response.text\n",
        "\n",
        "chars = list(set(data))\n",
        "data_size, vocab_size = len(data), len(chars)\n",
        "print(f'data has {data_size} characters, {vocab_size} unique.')\n",
        "\n",
        "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
        "ix_to_char = {i: ch for i, ch in enumerate(chars)}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pc3iAgLoLPl",
        "outputId": "a08afe29-dbeb-4bd7-c691-6609f43a2194"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data has 39580 characters, 88 unique.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "hidden_size = 100\n",
        "embedding_dim = 20\n",
        "seq_length = 25\n",
        "learning_rate = 1e-1\n",
        "batch_size = 20"
      ],
      "metadata": {
        "id": "F1bdC1IQoO7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = 0\n",
        "inputs = torch.tensor([char_to_ix[ch] for ch in data[p:p + seq_length]], dtype=torch.long).view(1, -1)\n",
        "targets = torch.tensor([char_to_ix[ch] for ch in data[p + 1:p + seq_length + 1]], dtype=torch.long).view(-1)"
      ],
      "metadata": {
        "id": "qsCmEz4SxA16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model definition\n",
        "class VanillaRNN(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
        "    super(VanillaRNN, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.i2h = nn.Linear(embedding_dim, hidden_size)\n",
        "    self.h2h = nn.Linear(hidden_size, hidden_size)\n",
        "    self.h2o = nn.Linear(hidden_size, vocab_size)\n",
        "    self.hb2 = nn.Parameter(torch.zeros(1, hidden_size))\n",
        "    self.ob = nn.Parameter(torch.zeros(1, vocab_size))\n",
        "\n",
        "  def forward(self, x, h):\n",
        "    x = self.embedding(x)\n",
        "    h2 = torch.tanh(self.i2h(x) + self.h2h(h) + self.hb2)\n",
        "    y = self.h2o(h2) + self.ob\n",
        "    return y, h2\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Model initialization\n",
        "model = VanillaRNN(vocab_size, embedding_dim, hidden_size)\n",
        "optimizer = optim.Adagrad(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "2nIoS_FgoXQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generateMiniBatch(start_idx):\n",
        "  batch_inputs = []\n",
        "  batch_targets = []\n",
        "\n",
        "  # Generate examples for the current minibatch\n",
        "  for i in range(batch_size):\n",
        "    p = start_idx + i\n",
        "    inputs = torch.tensor([char_to_ix[ch] for ch in data[p:p + seq_length]], dtype=torch.long).view(1, -1)\n",
        "    targets = torch.tensor([char_to_ix[ch] for ch in data[p + 1:p + seq_length + 1]], dtype=torch.long).view(-1)\n",
        "\n",
        "    batch_inputs.append(inputs)\n",
        "    batch_targets.append(targets)\n",
        "\n",
        "  # Convert lists to tensors\n",
        "  minibatch_inputs = torch.cat(batch_inputs, dim=0)\n",
        "  minibatch_targets = torch.cat(batch_targets, dim=0)\n",
        "  return minibatch_inputs, minibatch_targets"
      ],
      "metadata": {
        "id": "PgvNUMTTzp72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7miViwQnRUd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faa8b6db-f17d-4ebd-90fa-f551d3e954c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Loss: 5.486164093017578\n",
            "Iteration 100, Loss: 2.359689950942993\n",
            "Iteration 200, Loss: 2.285416841506958\n",
            "Iteration 300, Loss: 4.194525241851807\n",
            "Iteration 400, Loss: 3.3510031700134277\n",
            "Iteration 500, Loss: 2.848665952682495\n",
            "Iteration 600, Loss: 2.9528396129608154\n",
            "Iteration 700, Loss: 2.5791709423065186\n",
            "Iteration 800, Loss: 3.1590704917907715\n",
            "Iteration 900, Loss: 2.2458698749542236\n",
            "Iteration 1000, Loss: 2.4159185886383057\n",
            "Iteration 1100, Loss: 2.424057722091675\n",
            "Iteration 1200, Loss: 2.57066011428833\n",
            "Iteration 1300, Loss: 2.379361391067505\n",
            "Iteration 1400, Loss: 2.5716958045959473\n",
            "Iteration 1500, Loss: 2.441225051879883\n",
            "Iteration 1600, Loss: 2.617128849029541\n",
            "Iteration 1700, Loss: 2.248991012573242\n",
            "Iteration 1800, Loss: 2.251082420349121\n",
            "Iteration 1900, Loss: 2.705542802810669\n",
            "Iteration 2000, Loss: 2.1049044132232666\n",
            "Iteration 2100, Loss: 2.231717348098755\n",
            "Iteration 2200, Loss: 2.275906562805176\n",
            "Iteration 2300, Loss: 2.6123456954956055\n",
            "Iteration 2400, Loss: 2.596693515777588\n",
            "Iteration 2500, Loss: 2.112933874130249\n",
            "Iteration 2600, Loss: 1.9533435106277466\n",
            "Iteration 2700, Loss: 3.1088201999664307\n",
            "Iteration 2800, Loss: 2.3792178630828857\n",
            "Iteration 2900, Loss: 2.481163740158081\n",
            "Iteration 3000, Loss: 2.3139944076538086\n",
            "Iteration 3100, Loss: 1.8187068700790405\n",
            "Iteration 3200, Loss: 2.0055336952209473\n",
            "Iteration 3300, Loss: 1.8003324270248413\n",
            "Iteration 3400, Loss: 3.357168436050415\n",
            "Iteration 3500, Loss: 2.1688232421875\n",
            "Iteration 3600, Loss: 2.202040433883667\n",
            "Iteration 3700, Loss: 1.9777776002883911\n",
            "Iteration 3800, Loss: 2.692281723022461\n",
            "Iteration 3900, Loss: 3.1977100372314453\n",
            "Iteration 4000, Loss: 3.3212616443634033\n",
            "Iteration 4100, Loss: 2.947355031967163\n",
            "Iteration 4200, Loss: 2.475440263748169\n",
            "Iteration 4300, Loss: 2.5701558589935303\n",
            "Iteration 4400, Loss: 2.6921029090881348\n",
            "Iteration 4500, Loss: 2.4968786239624023\n",
            "Iteration 4600, Loss: 2.5503315925598145\n",
            "Iteration 4700, Loss: 2.1122562885284424\n",
            "Iteration 4800, Loss: 1.7836540937423706\n",
            "Iteration 4900, Loss: 1.8684598207473755\n",
            "Iteration 5000, Loss: 2.5621519088745117\n",
            "Iteration 5100, Loss: 2.565117835998535\n",
            "Iteration 5200, Loss: 3.114353656768799\n",
            "Iteration 5300, Loss: 2.568714141845703\n",
            "Iteration 5400, Loss: 1.9191035032272339\n",
            "Iteration 5500, Loss: 2.423089027404785\n",
            "Iteration 5600, Loss: 2.6522085666656494\n",
            "Iteration 5700, Loss: 2.4953808784484863\n",
            "Iteration 5800, Loss: 2.3319406509399414\n",
            "Iteration 5900, Loss: 2.59708571434021\n",
            "Iteration 6000, Loss: 1.3128809928894043\n",
            "Iteration 6100, Loss: 1.7278248071670532\n",
            "Iteration 6200, Loss: 2.5322186946868896\n",
            "Iteration 6300, Loss: 2.3813376426696777\n",
            "Iteration 6400, Loss: 1.9280856847763062\n",
            "Iteration 6500, Loss: 1.7754089832305908\n",
            "Iteration 6600, Loss: 2.3568341732025146\n",
            "Iteration 6700, Loss: 2.100858688354492\n",
            "Iteration 6800, Loss: 1.6191513538360596\n",
            "Iteration 6900, Loss: 2.7214410305023193\n",
            "Iteration 7000, Loss: 2.346132755279541\n",
            "Iteration 7100, Loss: 2.1992578506469727\n",
            "Iteration 7200, Loss: 2.2311978340148926\n",
            "Iteration 7300, Loss: 2.500234603881836\n",
            "Iteration 7400, Loss: 1.5742546319961548\n",
            "Iteration 7500, Loss: 2.3546688556671143\n",
            "Iteration 7600, Loss: 2.7030231952667236\n",
            "Iteration 7700, Loss: 2.6000053882598877\n",
            "Iteration 7800, Loss: 2.339721202850342\n",
            "Iteration 7900, Loss: 2.283750057220459\n",
            "Iteration 8000, Loss: 2.8551976680755615\n",
            "Iteration 8100, Loss: 2.178698778152466\n",
            "Iteration 8200, Loss: 1.7911843061447144\n",
            "Iteration 8300, Loss: 2.2398622035980225\n",
            "Iteration 8400, Loss: 1.8407527208328247\n",
            "Iteration 8500, Loss: 1.9059643745422363\n",
            "Iteration 8600, Loss: 1.9541698694229126\n",
            "Iteration 8700, Loss: 1.9138479232788086\n",
            "Iteration 8800, Loss: 1.5683478116989136\n",
            "Iteration 8900, Loss: 2.496317148208618\n",
            "Iteration 9000, Loss: 1.9891220331192017\n",
            "Iteration 9100, Loss: 3.128416061401367\n",
            "Iteration 9200, Loss: 1.9326480627059937\n",
            "Iteration 9300, Loss: 2.405402183532715\n",
            "Iteration 9400, Loss: 2.886293649673462\n",
            "Iteration 9500, Loss: 1.9492889642715454\n",
            "Iteration 9600, Loss: 2.138014793395996\n",
            "Iteration 9700, Loss: 2.527238130569458\n",
            "Iteration 9800, Loss: 2.444155693054199\n",
            "Iteration 9900, Loss: 2.1607155799865723\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "num_iterations = 10000\n",
        "for iteration in range(num_iterations):\n",
        "  # Prepare inputs\n",
        "  if iteration == 0 or p + seq_length + batch_size + 1 >= len(data):\n",
        "    hprev = torch.zeros(1, hidden_size)  # Reset RNN memory\n",
        "    p = 0  # Go from the start of data\n",
        "\n",
        "  # inputs = torch.tensor([char_to_ix[ch] for ch in data[p:p + seq_length]], dtype=torch.long).view(1, -1)\n",
        "  # targets = torch.tensor([char_to_ix[ch] for ch in data[p + 1:p + seq_length + 1]], dtype=torch.long).view(-1)\n",
        "\n",
        "  inputs, targets = generateMiniBatch(p)\n",
        "\n",
        "  # Forward pass\n",
        "  outputs, hprev = model(inputs, hprev)\n",
        "  loss = criterion(outputs.view(-1, vocab_size), targets)\n",
        "\n",
        "  # Backward pass\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  hprev = hprev.detach()\n",
        "\n",
        "  # Gradient clipping\n",
        "  for param in model.parameters():\n",
        "    if param.grad is not None:\n",
        "      param.grad.data.clamp_(-5, 5)\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "  # Print progress\n",
        "  if iteration % 100 == 0:\n",
        "    print(f'Iteration {iteration}, Loss: {loss.item()}')\n",
        "\n",
        "  p += batch_size  # Move data pointer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample from the model\n",
        "def sample(model, seed_ix, n):\n",
        "  h = torch.zeros(1, hidden_size)\n",
        "  x = torch.tensor(seed_ix, dtype=torch.long).view(1, 1)\n",
        "  ixes = []\n",
        "\n",
        "  for _ in range(n):\n",
        "    outputs, h = model(x, h)\n",
        "    p = nn.functional.softmax(outputs, dim=-1).detach().numpy().ravel()\n",
        "    ix = np.random.choice(range(vocab_size), p=p)\n",
        "    x = torch.tensor(ix, dtype=torch.long).view(1, 1)\n",
        "    ixes.append(ix)\n",
        "\n",
        "  return ixes"
      ],
      "metadata": {
        "id": "I9FANLvcoQtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate sample text\n",
        "sample_ix = sample(model, char_to_ix[data[0]], 2000)\n",
        "txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
        "print(f'Generated Text:\\n{txt}')"
      ],
      "metadata": {
        "id": "gIyG1ucAoVJE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
