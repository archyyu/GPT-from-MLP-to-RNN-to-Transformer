{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMC3ZUQikoPcPM2S6xuPjVp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/archyyu/RNN-GPT/blob/main/studyRNN3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vta7hPwLygow"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "#url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "url = \"https://raw.githubusercontent.com/archyyu/publicResource/main/google.dev.en\"\n",
        "#url = \"https://raw.githubusercontent.com/archyyu/publicResource/main/KDE4.en-es.en\"\n",
        "#url = \"https://raw.githubusercontent.com/archyyu/publicResource/main/js\"\n",
        "response = requests.get(url)\n",
        "data = response.text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "tcayE2XAwdpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(set(''.join(data)))\n",
        "i2c = dict(enumerate(chars))\n",
        "c2i = {c: i for i, c in i2c.items()}\n",
        "\n",
        "def encode(s):\n",
        "  il = []\n",
        "  for c in s:\n",
        "    il.append(c2i[c])\n",
        "  return il\n",
        "\n",
        "def decode(l):\n",
        "  cs = []\n",
        "  for i in l:\n",
        "    cs.append(i2c[i])\n",
        "  return ''.join(cs)"
      ],
      "metadata": {
        "id": "G-yraDchyk4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "K6jVgdgEywaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_size, output_size):\n",
        "    super(RNN, self).__init__()\n",
        "    self.emb = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.i2h = nn.Linear(embedding_dim, hidden_size)\n",
        "    self.h2h = nn.Linear(hidden_size, hidden_size)\n",
        "    self.h2o = nn.Linear(hidden_size, output_size)\n",
        "    self.bh = nn.Parameter(torch.zeros(hidden_size, 1,dtype=torch.float))\n",
        "    self.bo = nn.Parameter(torch.zeros(output_size, 1,dtype=torch.float))\n",
        "\n",
        "  def forward(self, input, hidden):\n",
        "    embedding = self.emb(input)\n",
        "    fist = self.i2h(embedding)\n",
        "    second = self.h2h(hidden)\n",
        "    print(fist.shape)\n",
        "    print(second.shape)\n",
        "    h = torch.tanh(fist + second + self.bh)\n",
        "    y = self.h2o(h) + self.bo\n",
        "    return y, h\n"
      ],
      "metadata": {
        "id": "CK8uLqai16M2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoader(object):\n",
        "\n",
        "  def __init__(self, data, seq_length):\n",
        "    self.data = data\n",
        "    self.seq_length = seq_length\n",
        "    self.chars = list(set(data))\n",
        "    self.char_to_ix = {ch: i for i, ch in enumerate(self.chars)}\n",
        "    self.ix_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
        "\n",
        "  def __iter__(self):\n",
        "    for i in range(len(self.data) - self.seq_length + 1):\n",
        "      inputs = torch.tensor([self.char_to_ix[ch] for ch in self.data[i:i + self.seq_length]],dtype=torch.long)\n",
        "      targets = torch.tensor([self.char_to_ix[ch] for ch in self.data[i + 1:i + self.seq_length + 1]],dtype=torch.float)\n",
        "      yield inputs, targets"
      ],
      "metadata": {
        "id": "aDLzPGx_82k3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 25\n",
        "data_loader = DataLoader(data, seq_length)"
      ],
      "metadata": {
        "id": "VGu4IOWj98FA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in data_loader:\n",
        "  print(\"input:\",inputs)\n",
        "  print(\"output:\",targets)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyZKxoyX-aaR",
        "outputId": "5b28a663-3810-40c1-e8b6-df87616df8ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input: tensor([55, 16, 17, 34,  9,  0, 54, 16,  9, 16, 23, 19, 49,  2, 41, 57, 19,  4,\n",
            "        46, 17, 19,  0, 45, 19,  0])\n",
            "output: tensor([16., 17., 34.,  9.,  0., 54., 16.,  9., 16., 23., 19., 49.,  2., 41.,\n",
            "        57., 19.,  4., 46., 17., 19.,  0., 45., 19.,  0., 62.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars = list(set(data))\n",
        "len(chars)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvOEm7LpDLV9",
        "outputId": "c74fb799-4def-4790-d30c-63d2fc69c4cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_hidden = 128\n",
        "embedding_dim = 10\n",
        "chars = list(set(data))\n",
        "rnn = RNN(len(chars), embedding_dim, n_hidden, len(chars))\n",
        "learning_rate = 0.0005\n",
        "optimizer = optim.Adam(rnn.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "n_epochs = 2000\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  h_prev = torch.zeros(n_hidden, n_hidden)\n",
        "  for inputs, targets in data_loader:\n",
        "    optimizer.zero_grad()\n",
        "    outputs, h_prev = rnn(inputs, h_prev)\n",
        "    loss = loss_fn(outputs, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  if epoch % 1000 == 0:\n",
        "    print(loss)\n"
      ],
      "metadata": {
        "id": "lyGgaj6618aJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "outputId": "a7f56af6-1ee6-4726-a420-b1ca5f4a7e5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([25, 128])\n",
            "torch.Size([128, 128])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-1a775801ba2a>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_prev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-4f97e9191324>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msecond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfist\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msecond\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh2o\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (25) must match the size of tensor b (128) at non-singleton dimension 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the vocabulary size and embedding dimension\n",
        "vocab_size = 65  # Number of unique items (e.g., words)\n",
        "embedding_dim = 128  # Dimensionality of the embedding space\n",
        "\n",
        "# Create an instance of nn.Embedding\n",
        "embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
        "\n",
        "# Generate some example indices (batch size = 3, sequence length = 2)\n",
        "indices = torch.tensor([17., 61., 34., 25., 28.,  6., 27., 61., 28., 61., 33., 41.,  1., 53.,\n",
        "        50., 56., 41.,  7., 58., 34., 41.,  6., 47., 41.,  6.], dtype=torch.long)\n",
        "\n",
        "# Use the embedding layer to get dense representations for the indices\n",
        "embeddings = embedding_layer(indices)\n",
        "\n",
        "# Print the result\n",
        "print(\"Embeddings:\")\n",
        "print(embeddings)\n",
        "embeddings.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAzBNGdwFu9Z",
        "outputId": "6b2df343-71fb-4c63-b320-954941c3c6f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings:\n",
            "tensor([[-0.9908, -0.6424, -0.2864,  ...,  0.4558, -0.2979,  0.9670],\n",
            "        [ 0.0865, -0.5986, -0.5198,  ..., -0.3778, -0.2373,  0.4017],\n",
            "        [ 0.3628,  0.4025,  1.1648,  ..., -0.3780,  1.0158, -0.8076],\n",
            "        ...,\n",
            "        [-0.3223, -0.1076, -2.0110,  ...,  1.2723,  0.1564,  0.4164],\n",
            "        [-0.6555, -0.4614, -1.4212,  ...,  0.0568, -0.5009,  0.4072],\n",
            "        [-1.2549,  0.4909,  1.5384,  ...,  0.6061, -0.2331, -1.3316]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([25, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    }
  ]
}