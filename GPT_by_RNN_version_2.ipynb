{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM08ojleoyOgCyQ/SN+kNaN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/archyyu/GPT-from-MLP-to-RNN-to-Transformer/blob/main/GPT_by_RNN_version_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.nn import functional as F\n",
        "import matplotlib.pyplot as plt # for making figures\n",
        "%matplotlib inline\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "id": "uOcLhTikoHXs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "092dd09e-bd1e-4304-ac09-61f06d7f58c8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7df46ef943d0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data I/O\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "#url = \"https://raw.githubusercontent.com/archyyu/publicResource/main/google.dev.en\"\n",
        "#url = \"https://raw.githubusercontent.com/torvalds/linux/master/mm/madvise.c\"\n",
        "response = requests.get(url)\n",
        "data = response.text\n",
        "\n",
        "chars = list(set(data))\n",
        "data_size, vocab_size = len(data), len(chars)\n",
        "print(f'data has {data_size} characters, {vocab_size} unique.')\n",
        "\n",
        "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
        "ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "n =  (int)(0.9*len(data))\n",
        "training_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pc3iAgLoLPl",
        "outputId": "163a88d0-5a08-48d8-e142-2ccd61371308"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data has 1115394 characters, 65 unique.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "hidden_size = 100\n",
        "embedding_dim = 20\n",
        "seq_length = 25\n",
        "learning_rate = 1e-1\n",
        "batch_size = 20\n",
        "eval_iters = 200"
      ],
      "metadata": {
        "id": "F1bdC1IQoO7B"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ManillaRNN(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
        "    super(ManillaRNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.i2h = nn.Linear(embedding_dim, hidden_size)\n",
        "    self.h2h = nn.Linear(hidden_size, hidden_size)\n",
        "    self.h2o = nn.Linear(hidden_size, vocab_size)\n",
        "    self.hb2 = nn.Parameter(torch.zeros(1, hidden_size))\n",
        "    self.ob = nn.Parameter(torch.zeros(1, vocab_size))\n",
        "\n",
        "  def forward(self, x, targets):\n",
        "    h = torch.zeros(1, self.hidden_size)\n",
        "    y_list = []\n",
        "    for i in range(x.shape[1]):\n",
        "      t = self.embedding(x[:,i])\n",
        "      h = torch.tanh(self.i2h(t) + self.h2h(h) + self.hb2)\n",
        "      y = self.h2o(h) + self.ob\n",
        "      y_list.append(y)\n",
        "    predicts = torch.stack(y_list, dim=1);\n",
        "\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B,T = targets.shape\n",
        "      loss = F.cross_entropy(predicts.view(B*T, -1), targets.view(B*T))\n",
        "    return predicts, loss\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model = ManillaRNN(vocab_size, embedding_dim, hidden_size)\n",
        "optimizer = optim.Adagrad(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "PIhImafKpZNI"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now tusi ManillaRNN is different with the VanillaRNN.\n",
        "It will iterate all the time step, drop the intermediate output, and only output the final one.\n",
        "\n",
        "But I am not going to rewrite the training function to retrain the new model.\n",
        "Because I think the VanillaRNN is more better and controlable than this one.\n"
      ],
      "metadata": {
        "id": "aHBdvi-upC5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getBatch(split):\n",
        "  batch_inputs = []\n",
        "  batch_targets = []\n",
        "\n",
        "  data = training_data if split == 'train' else val_data\n",
        "  start_idx = torch.randint(len(data) - batch_size - seq_length - 2,[1]).item()\n",
        "\n",
        "  # Generate examples for the current minibatch\n",
        "  for i in range(batch_size):\n",
        "    p = start_idx + i\n",
        "    inputs = torch.tensor([char_to_ix[ch] for ch in data[p:p + seq_length]], dtype=torch.long).view(1, -1)\n",
        "    targets = torch.tensor([char_to_ix[ch] for ch in data[p + 1:p + seq_length + 1]], dtype=torch.long).view(-1)\n",
        "\n",
        "    batch_inputs.append(inputs)\n",
        "    batch_targets.append(targets)\n",
        "\n",
        "  # Convert lists to tensors\n",
        "  minibatch_inputs = torch.cat(batch_inputs, dim=0)\n",
        "  minibatch_targets = torch.stack(batch_targets)\n",
        "  return minibatch_inputs, minibatch_targets\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = getBatch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "PgvNUMTTzp72"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "t7miViwQnRUd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec104903-4ad4-4a43-dae2-164bd4949c73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 1.8895, val loss 1.9674\n",
            "step 1000: train loss 1.8635, val loss 1.9978\n",
            "step 2000: train loss 1.8343, val loss 2.0014\n",
            "step 3000: train loss 1.8512, val loss 1.9489\n",
            "step 4000: train loss 1.8565, val loss 1.9407\n",
            "step 5000: train loss 1.7965, val loss 1.9693\n",
            "step 6000: train loss 1.8494, val loss 1.9797\n",
            "step 7000: train loss 1.8748, val loss 1.9767\n",
            "step 8000: train loss 1.8604, val loss 1.9742\n",
            "step 9000: train loss 1.8603, val loss 1.9759\n",
            "step 10000: train loss 1.8498, val loss 1.9463\n",
            "step 11000: train loss 1.7943, val loss 1.9614\n",
            "step 12000: train loss 1.8332, val loss 1.9412\n",
            "step 13000: train loss 1.8238, val loss 1.9847\n",
            "step 14000: train loss 1.8174, val loss 1.9862\n",
            "step 15000: train loss 1.8433, val loss 1.9471\n",
            "step 16000: train loss 1.7991, val loss 1.9472\n",
            "step 17000: train loss 1.8069, val loss 1.9311\n",
            "step 18000: train loss 1.8141, val loss 1.9344\n",
            "step 19000: train loss 1.7987, val loss 1.9529\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "num_iterations = 20000\n",
        "for iteration in range(num_iterations):\n",
        "\n",
        "  inputs, targets = getBatch('train')\n",
        "  predicts, loss = model(inputs, None)\n",
        "  optimizer.zero_grad()\n",
        "  B,T = targets.shape\n",
        "\n",
        "  totalloss = criterion(predicts.view(B*T, -1), targets.view(B*T))\n",
        "  totalloss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if iteration % 1000 == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iteration}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "\n",
        "  start = ['F']\n",
        "  result = ['F']\n",
        "\n",
        "  for i in range(2000):\n",
        "    start = start[-seq_length:]\n",
        "    ilist = torch.tensor([char_to_ix[i] for i in start])\n",
        "    ilist = ilist.reshape(1, -1)\n",
        "    outputs, loss = model(ilist, None)\n",
        "\n",
        "    p = nn.functional.softmax(outputs[:,-1,:], dim=-1).detach().numpy().ravel()\n",
        "    ix = np.random.choice(range(vocab_size), p=p)\n",
        "    start.append(ix_to_char[ix])\n",
        "    result.append(ix_to_char[ix])\n",
        "\n",
        "  print(''.join(result))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUF9pT7OHdwM",
        "outputId": "213478de-ce1b-4ea0-bb1e-1eac1245b754"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fon him Lond the mustion:\n",
            "Is nevicand fait shay,\n",
            "To dreainst scrriend',\n",
            "I in tain.\n",
            "\n",
            "WICH:\n",
            "Mand sead, sure:\n",
            "And tat.\n",
            "\n",
            "TONCONKEDS LORS:\n",
            "My Wooke in aclecs?\n",
            "\n",
            "Feet livawe?\n",
            "\n",
            "SICHARD:\n",
            "In herh VION MAUTIUC\n",
            "ERTY:\n",
            "Was rewy!\n",
            "\n",
            "FLORIZARE:\n",
            "My from should A and quick'd,\n",
            "The wither my lagman! Beg cans.\n",
            "\n",
            "NISIA:\n",
            "Than dound will draot cone say.\n",
            "\n",
            "Nather in's than curd:\n",
            "Where your here plutise sham brem, I am arwenckired\n",
            "Mit!\n",
            "\n",
            "PRINCENT:\n",
            "Gyim, then very shan have it batimies the she his in't misiniup\n",
            "Yis itwards-\n",
            "Thus I will with them loverilesly soad in then at An neven. Kinster you, from the\n",
            "Deoming some\n",
            "prined MICHAPET:\n",
            "NoRm.\n",
            " the word ben may this follintn nike thence are go\n",
            "Hest shall trut\n",
            "That so.\n",
            "I came.\n",
            "\n",
            "RUEEONENCE:\n",
            "When-us,\n",
            "Pof him noter.\n",
            "Lfright,\n",
            "And\n",
            "be is reat my qunolbst surnics sorte; lives!\n",
            "His of dost beep of this senzen the whithe soolue, to enother:\n",
            "When the liruter's will alming.\n",
            "That.\n",
            "\n",
            "CORIOLANUS:\n",
            "Well hay wiffult\n",
            "And think, signing-Lay-pineds al liver, those;\n",
            "This, though death\n",
            "Dy brolevoud shaly.\n",
            "\n",
            "CORIST loves? Lenple.\n",
            "\n",
            "HaThoicionds on it fangenacion, britse that tould hearsors!\n",
            "But on can thy did but niviom trous\n",
            "Noth'd ender,\n",
            "And on me with all my thou jove you, worth then prante of quro, thom naterve What affendeds troweding;\n",
            "For gland the enevery's be know than I drahe in my Splild.\n",
            "\n",
            "MENENIUS:\n",
            "I make whan word?\n",
            "Pimile here all to head he like; cirds; parid that.\n",
            "\n",
            "DUCENNEO:\n",
            "E's not of on blood omed his my mose lixtly wonte.\n",
            "\n",
            "JXONTCHICS:\n",
            "I: thy chaid.\n",
            "Theech gownntixcel nobs\n",
            "Where?\n",
            "\n",
            "KING RICHARD:\n",
            "Sen;\n",
            "To tookence wass lory made benton honour.\n",
            "\n",
            "WARLEY:\n",
            "A proull didic! my hunt chough Lord,\n",
            "Iwameting I challer, ry.\n",
            "\n",
            "ROMEO:\n",
            "\n",
            "FLORIZEL:\n",
            "Well body on your alay nemps the morm his see was the greathers.\n",
            "Lends that on, thbtice, Coust to lisbith her.\n",
            "\n",
            "GLOUCESTER:\n",
            "Hen be dand? Put there thus unnuter; am pand!\n",
            "\n",
            "First bulias that love chermer.\n",
            "\n",
            "JUFENCEL:\n",
            "Orfold.\n",
            "\n",
            "wath---mand, though,\n",
            "And fain the girn so?\n",
            "\n",
            "HORS BRIAS:\n",
            "Dit deadly grointaneswers fastade\n",
            "the candovemment'd all winj\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the RNN is better for seq to seq.but it also better for seq to 1.\n",
        "In the above example, it is actually a example of seq to 1.\n",
        "The thing is that, I only use the last output of the node as the result of the rnn.\n",
        "\n",
        "I am not sure what if I stack all the outputs of the nodes, and squeese them tegether to get the final result is better or not. will continue to explore.\n",
        "\n",
        "I think there is also another way to optimize the above model.\n",
        "When I train the model, I prepare my data like this, the input is a list of characters, the expected output is one character.\n",
        "but we could also prepare data like this, the input a list of characters, the expect result is also a list of character, but with one step shift.\n",
        "but when generating data, I could just use the last character as the result.\n",
        "\n",
        "I will try the last one later."
      ],
      "metadata": {
        "id": "tZzxsUe8AdRw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The new idea is that.\n",
        "\n",
        "I always use the last time step as the final output of the RNN, or as the predict character of the previous input. we could go further as that, add attention in the final step, to focus differently on the previous tokens to decide what to predict, let do that."
      ],
      "metadata": {
        "id": "UscbtqGkCQWy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this new version, I split the data into training data and val data\n",
        "So after some iteration, we could compare the training error with val error to see if we have overfitted the model."
      ],
      "metadata": {
        "id": "uZHmznhvGDfX"
      }
    }
  ]
}